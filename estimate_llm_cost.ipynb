{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chepurova/Text2KG/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from utils.openai_utils import LLMTripletExtractor\n",
    "from utils.structured_dynamic_index_utils import Aligner\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis\t\t\t   musique.json\n",
      "attempt2res.json\t\t   musique_res\n",
      "attempt2res_musique.json\t   musique_res_gpt-4.1\n",
      "create_indexes.py\t\t   musique_structured_inference.py\n",
      "estimate_llm_cost.ipynb\t\t   musique_updated_res\n",
      "hotpot_gpt_4.1\t\t\t   musique_updated_res_9_04\n",
      "hotpot_gpt_4.1-mini\t\t   musique_updated_res_gpt_4.1-mini\n",
      "hotpot_gpt-4o-mini\t\t   musique_updated_res_gpt4o_mini\n",
      "hotpotqa200.json\t\t   pipeline_test.ipynb\n",
      "hotpot_qa_structured_inference.py  populate_db.py\n",
      "kg-from-wiki-dynamic.ipynb\t   preprocessing\n",
      "kg-from-wiki.ipynb\t\t   requirements.txt\n",
      "llama-musique_openrouter_test\t   setup_db.sh\n",
      "logs\t\t\t\t   synthie-tests.ipynb\n",
      "mongo_test.ipynb\t\t   utils\n",
      "musique_200_test.json\t\t   venv\n",
      "musique_gpt4o_mini\t\t   wikidata_vs_text2kg.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for file in os.listdir(\"hotpot_gpt-4o-mini\"):\n",
    "    if \"final\" in file:\n",
    "        c += 1\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostCalc:\n",
    "    model_pricing = {\n",
    "        \"gpt-4o-mini\": (0.15, 0.6),\n",
    "        \"gpt-4o\": (5.0, 15.0),\n",
    "    }\n",
    "\n",
    "    def __init__(self, model: str) -> None:\n",
    "        self.model = model\n",
    "        self.encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    def count_tokens(self, content: str):\n",
    "        return len(self.encoding.encode(content))\n",
    "\n",
    "    def calculate_input_cost(self, content: str):\n",
    "        return (\n",
    "            self.model_pricing[self.model][0] / 10**6 * self.count_tokens(content)\n",
    "        )\n",
    "\n",
    "    def calculate_output_cost(self, content: str):\n",
    "        return (\n",
    "            self.model_pricing[self.model][1] / 10**6 * self.count_tokens(content)\n",
    "        )\n",
    "\n",
    "\n",
    "calc = CostCalc(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = Aligner()\n",
    "model_name = 'gpt-4o'\n",
    "extractor = LLMTripletExtractor(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Borisov is up for Best Supporting Actor for his role in Sean Baker's “Anora” at the Academy Awards on March 2, making him the first Russian to be nominated in an acting category since the fall of the Soviet Union.\"\n",
    "# input_1st_step = extractor.extract_triplets_from_text(text)\n",
    "# system_prompt = input_1st_step['system_prompt']\n",
    "# user_prompt = input_1st_step['user_prompt']\n",
    "\n",
    "# calc.calculate_input_cost(system_prompt) + calc.calculate_input_cost(user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"hotpotqa200.json\", \"r\") as f:\n",
    "    ds = json.load(f)\n",
    "\n",
    "id2sample = {}\n",
    "for elem in ds:\n",
    "    id2sample[elem['_id']] = elem\n",
    "len(id2sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '5a7613c15542994ccc9186bf',\n",
       " 'answer': 'Gesellschaft mit beschränkter Haftung',\n",
       " 'question': \"VIVA Media AG changed it's name in 2004. What does their new acronym stand for?\",\n",
       " 'supporting_facts': [['VIVA Media', 0],\n",
       "  ['Gesellschaft mit beschränkter Haftung', 0]],\n",
       " 'context': [['Constantin Medien',\n",
       "   ['Constantin Medien AG (formerly EM.Entertainment and EM.TV & Merchandising AG, then EM.TV AG, and finally em.sport media ag) is a German media group, based in Ismaning near Munich, active in the area of sports, film and event marketing to medium-sized media companies.']],\n",
       "  ['VIVA Poland',\n",
       "   ['VIVA Polska (earlier \"VIVApolska!\")',\n",
       "    ' is a Polish 24h music and entertainment channel from Viacom International Media Networks Polska.',\n",
       "    ' The channel was officially launched on June 10, 2000 by the German VIVA Media AG.']],\n",
       "  ['Viva (UK and Ireland)',\n",
       "   ['Viva (stylised as VIVA) is a music television channel in the United Kingdom and Ireland, owned by VIVA Media and thereby Viacom International Media Networks Europe.',\n",
       "    ' The channel launched on 26 October 2009, replacing TMF.']],\n",
       "  ['Blic',\n",
       "   ['Blic (Cyrillic: Блиц, ] ) is a daily middle-market tabloid newspaper in Serbia.',\n",
       "    ' Founded in 1996, \"Blic\" is owned by Ringier Axel Springer Media AG, a joint venture between Ringier media corporation from Switzerland and Axel Springer AG from Germany.']],\n",
       "  ['Qontis',\n",
       "   ['Qontis is a Switzerland based online personal finance management (PFM) platform.',\n",
       "    ' The service is part of a commercial enterprise between the \"Neue Zürcher Zeitung\" media property and e-banking solutions provider Crealogix.',\n",
       "    ' The platform provides users with the ability to document and organize data from all instances of private income and expenditures.',\n",
       "    \" Qontis' CEO (chief executive officer) is Christian Bieri, who formerly served as the Austrian Country Manager and CEE for the Vienna branch of Avaloq Evolution AG.\",\n",
       "    \" The company's CMO (chief marketing officer) is Nils Reimelt, the former digital director at Ringier Axel Springer Media AG.\"]],\n",
       "  ['VIVA Media',\n",
       "   ['VIVA Media GmbH (until 2004 \"VIVA Media AG\") is a music television network originating from Germany.',\n",
       "    ' It was founded for broadcast of VIVA Germany as VIVA Media AG in 1993 and has been owned by their original concurrent Viacom, the parent company of MTV, since 2004.',\n",
       "    ' Viva channels exist in some European countries; the first spin-offs were launched in Poland and Switzerland in 2000.']],\n",
       "  ['ProSiebenSat.1 Media',\n",
       "   ['ProSiebenSat.1 Media SE (officially abbreviated as P7S1, formerly ProSiebenSat.1 Media AG) is a European mass media company, based in Germany.',\n",
       "    ' It operates free-to-air commercial TV channels, pay TV channels, radio stations and related print businesses.',\n",
       "    ' It was formed on October 2, 2000 by the merger of German TV broadcasters ProSieben Media AG (founded in 1989) and Sat.1 SatellitenFernsehen GmbH (founded in 1984 as PKS (Programmgesellschaft für Kabel- und Satellitenrundfunk)).',\n",
       "    ' The company is listed on the Frankfurt Stock Exchange and is a component of the DAX index.']],\n",
       "  ['Gesellschaft mit beschränkter Haftung',\n",
       "   ['A Gesellschaft mit beschränkter Haftung (] , abbreviated GmbH ] and also GesmbH in Austria) is a type of legal entity very common in Germany, Austria, Switzerland (where it is equivalent to a S.à r.l.) and Liechtenstein.',\n",
       "    ' In the United States, the equivalent type of entity is the limited liability company (LLC).',\n",
       "    ' The name of the GmbH form emphasizes the fact that the owners (\"Gesellschafter\", also known as members) of the entity are not personally liable for the company\\'s debts.',\n",
       "    ' \"GmbH\"s are considered legal persons under German and Austrian law.',\n",
       "    ' Other variations include mbH (used when the term \"Gesellschaft\" is part of the company name itself), and gGmbH (\"gemeinnützige\" GmbH) for non-profit companies.']],\n",
       "  ['Mix Megapol',\n",
       "   ['Mix Megapol is a private Swedish radio network controlled by ProSiebenSat.1 Media AG.',\n",
       "    ' It launched in 1993 under the name Skärgårdsradion (Archipelago Radio).',\n",
       "    ' Later that year the name was changed to Radio Megapol when the broadcasting permissions were auctioned out.',\n",
       "    ' In 1997 the word \"Mix\" was added and their slogan became \"The best mix of hits and oldies\".',\n",
       "    ' Mix Megapol is on air in 24 cities from Kiruna in the north to Malmö in the south.',\n",
       "    ' They have over two million listeners per week.',\n",
       "    ' Their target group is people aged between 25 and 45.']],\n",
       "  ['John M. Keller',\n",
       "   ['John M. Keller (born March 5, 1938) is an American educational psychologist.',\n",
       "    ' He is best known for his work on motivation in educational settings and in particular the ARCS model of instructional design.',\n",
       "    ' The four elements of the acronym stand for Attention, Relevance, Confidence and Satisfaction (ARCS).']]],\n",
       " 'type': 'bridge',\n",
       " 'level': 'hard'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:55<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "COST = 0\n",
    "for sample_id in tqdm(id2sample):\n",
    "\n",
    "    sample = id2sample[sample_id]\n",
    "\n",
    "    aligner = Aligner()\n",
    "    model_name = 'gpt-4o'\n",
    "    extractor = LLMTripletExtractor(model=model_name)\n",
    "    \n",
    "    texts = [\" \".join(item[1]) for item in sample['context']]\n",
    "    for text in texts:\n",
    "        input_1st_step = extractor.extract_triplets_from_text(text)\n",
    "        system_prompt = input_1st_step['system_prompt']\n",
    "        user_prompt = input_1st_step['user_prompt']\n",
    "\n",
    "        COST += calc.calculate_input_cost(system_prompt) + calc.calculate_input_cost(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.599879999999992"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 152.03it/s]\n"
     ]
    }
   ],
   "source": [
    "triplets_num = []\n",
    "output_costs = []\n",
    "id2answer = {}\n",
    "for file in tqdm(os.listdir('musique_updated_res_9_04'), total=100):    \n",
    "    if 'final' in file:\n",
    "        df = pd.read_csv('musique_updated_res_9_04/{}'.format(file), index_col=0)\n",
    "        rows_as_dict = df[[\"subject\", \"relation\", \"object\", \"subject_type\", \"object_type\", \"qualifiers\"]].to_dict(orient='records')\n",
    "        triplets_num.extend(df['source_text_ids'].value_counts().tolist())\n",
    "        \n",
    "        output_costs.extend([calc.calculate_output_cost(json.dumps(row)) for row in rows_as_dict])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2240000000000002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(triplets_num) * np.median(output_costs) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.156000000000002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(triplets_num) * np.max(output_costs) * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(text):\n",
    "    \"\"\"Extract and refine knowledge graph triplets from text using LLM.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to extract triplets from\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (final_triplets, filtered_triplets) where:\n",
    "            - final_triplets: List of validated and refined triplets\n",
    "            - filtered_triplets: List of triplets that couldn't be validated\n",
    "    \"\"\"\n",
    "    # Extract initial triplets using LLM\n",
    "    COST = 0\n",
    "    extracted_triplets = extractor.extract_triplets_from_text(text)\n",
    "    \n",
    "\n",
    "    for triplet in extracted_triplets['triplets']:\n",
    "        # print(\"1st step triplet: \", triplet)\n",
    "        \n",
    "        # Get candidate entity types\n",
    "        subj_type_ids, obj_type_ids = aligner.retrieve_similar_entity_types(triplet=triplet)\n",
    "        \n",
    "        # Get candidate properties/relations\n",
    "        properties = aligner.retrieve_properties_for_entity_type(\n",
    "            target_relation=triplet['relation'],\n",
    "            object_types=obj_type_ids, \n",
    "            subject_types=subj_type_ids,\n",
    "            k=5\n",
    "        )\n",
    "\n",
    "        # Build candidate triplet backbones\n",
    "        # print(properties)\n",
    "        candidates = []\n",
    "        for prop_id, prop_label, prop_direction in properties:\n",
    "            if prop_direction == 'direct':\n",
    "                subject_types = set(subj_type_ids) & set(aligner.prop2constraints[prop_id]['Subject type constraint'])\n",
    "                object_types = set(obj_type_ids) & set(aligner.prop2constraints[prop_id]['Value-type constraint'])\n",
    "            else:\n",
    "                object_types = set(subj_type_ids) & set(aligner.prop2constraints[prop_id]['Value-type constraint']) \n",
    "                subject_types = set(obj_type_ids) & set(aligner.prop2constraints[prop_id]['Subject type constraint'])\n",
    "\n",
    "            # Use original type sets if no constraints matched\n",
    "            subject_types = subj_type_ids if len(subject_types) == 0 else subject_types\n",
    "            object_types = obj_type_ids if len(object_types) == 0 else object_types\n",
    "\n",
    "            candidates.append({\n",
    "                \"subject\": triplet['subject'] if prop_direction == 'direct' else triplet['object'],\n",
    "                \"relation\": prop_label,\n",
    "                'object': triplet['object'] if prop_direction == 'direct' else triplet['subject'],\n",
    "                \"subject_types\": [aligner.entity_type2label[t] for t in subject_types],\n",
    "                \"object_types\": [aligner.entity_type2label[t] for t in object_types]\n",
    "            })\n",
    "\n",
    "            # print({\n",
    "            #     \"subject\": triplet['subject'] if prop_direction == 'direct' else triplet['object'],\n",
    "            #     \"relation\": prop_label,\n",
    "            #     'object': triplet['object'] if prop_direction == 'direct' else triplet['subject'],\n",
    "            #     \"subject_types\": [aligner.entity_type2label[t] for t in subject_types],\n",
    "            #     \"object_types\": [aligner.entity_type2label[t] for t in object_types]\n",
    "            # })\n",
    "\n",
    "\n",
    "        # Refine relation and entity types using LLM - choose among valid backbones for triplet\n",
    "        input_2nd_step =  extractor.refine_relation_and_entity_types(\n",
    "            text=text, \n",
    "            triplet=triplet,\n",
    "            candidate_triplets=candidates\n",
    "        )\n",
    "        system_prompt = input_2nd_step['system_prompt']\n",
    "        user_prompt = input_2nd_step['user_prompt']\n",
    "\n",
    "        COST += calc.calculate_input_cost(system_prompt) + calc.calculate_input_cost(user_prompt)\n",
    "\n",
    "    return COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:55<00:00,  5.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3911350000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COST = 0\n",
    "for sample_id in id2sample:\n",
    "\n",
    "    sample = id2sample[sample_id]\n",
    "\n",
    "    aligner = Aligner()\n",
    "    model_name = 'gpt-4o'\n",
    "    extractor = LLMTripletExtractor(model=model_name)\n",
    "    \n",
    "    texts = [item['paragraph_text'] for item in sample['paragraphs']]\n",
    "    for text in tqdm(texts):        \n",
    "        COST += extract_triplets(text)\n",
    "\n",
    "    break\n",
    "\n",
    "COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.71562"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COST * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(text):\n",
    "    COST = 0\n",
    "    \"\"\"Extract and refine knowledge graph triplets from text using LLM.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to extract triplets from\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (final_triplets, filtered_triplets) where:\n",
    "            - final_triplets: List of validated and refined triplets\n",
    "            - filtered_triplets: List of triplets that couldn't be validated\n",
    "    \"\"\"\n",
    "    # Extract initial triplets using LLM\n",
    "    extracted_triplets = extractor.extract_triplets_from_text(text)\n",
    "    \n",
    "    final_triplets = []\n",
    "    filtered_triplets = []\n",
    "\n",
    "    for triplet in extracted_triplets['triplets']:\n",
    "        # print(\"1st step triplet: \", triplet)\n",
    "        \n",
    "        # Get candidate entity types\n",
    "        subj_type_ids, obj_type_ids = aligner.retrieve_similar_entity_types(triplet=triplet)\n",
    "        \n",
    "        # Get candidate properties/relations\n",
    "        properties = aligner.retrieve_properties_for_entity_type(\n",
    "            target_relation=triplet['relation'],\n",
    "            object_types=obj_type_ids, \n",
    "            subject_types=subj_type_ids,\n",
    "            k=5\n",
    "        )\n",
    "\n",
    "        # Build candidate triplet backbones\n",
    "        # print(properties)\n",
    "        candidates = []\n",
    "        for prop_id, prop_label, prop_direction in properties:\n",
    "            if prop_direction == 'direct':\n",
    "                subject_types = set(subj_type_ids) & set(aligner.prop2constraints[prop_id]['Subject type constraint'])\n",
    "                object_types = set(obj_type_ids) & set(aligner.prop2constraints[prop_id]['Value-type constraint'])\n",
    "            else:\n",
    "                object_types = set(subj_type_ids) & set(aligner.prop2constraints[prop_id]['Value-type constraint']) \n",
    "                subject_types = set(obj_type_ids) & set(aligner.prop2constraints[prop_id]['Subject type constraint'])\n",
    "\n",
    "            # Use original type sets if no constraints matched\n",
    "            subject_types = subj_type_ids if len(subject_types) == 0 else subject_types\n",
    "            object_types = obj_type_ids if len(object_types) == 0 else object_types\n",
    "\n",
    "            candidates.append({\n",
    "                \"subject\": triplet['subject'] if prop_direction == 'direct' else triplet['object'],\n",
    "                \"relation\": prop_label,\n",
    "                'object': triplet['object'] if prop_direction == 'direct' else triplet['subject'],\n",
    "                \"subject_types\": [aligner.entity_type2label[t] for t in subject_types],\n",
    "                \"object_types\": [aligner.entity_type2label[t] for t in object_types]\n",
    "            })\n",
    "\n",
    "            # print({\n",
    "            #     \"subject\": triplet['subject'] if prop_direction == 'direct' else triplet['object'],\n",
    "            #     \"relation\": prop_label,\n",
    "            #     'object': triplet['object'] if prop_direction == 'direct' else triplet['subject'],\n",
    "            #     \"subject_types\": [aligner.entity_type2label[t] for t in subject_types],\n",
    "            #     \"object_types\": [aligner.entity_type2label[t] for t in object_types]\n",
    "            # })\n",
    "\n",
    "\n",
    "        # Refine relation and entity types using LLM - choose among valid backbones for triplet\n",
    "        backbone_triplet = extractor.refine_relation_and_entity_types(\n",
    "            text=text, \n",
    "            triplet=triplet,\n",
    "            candidate_triplets=candidates\n",
    "        )\n",
    "        backbone_triplet['qualifiers'] = triplet['qualifiers']\n",
    "\n",
    "        # Refine entity names\n",
    "        COST += refine_entities(text, backbone_triplet, aligner)\n",
    "        \n",
    "\n",
    "\n",
    "    # print(\"-\"*100)\n",
    "    return COST\n",
    "\n",
    "\n",
    "def refine_entities(text, triplet, aligner):\n",
    "    \"\"\"Refine entity names using type constraints.\"\"\"\n",
    "    # Handle object refinement\n",
    "    obj_type = triplet['object_type']\n",
    "    obj_type_id = aligner.label2entity_type.get(obj_type, '')\n",
    "    obj_hierarchy = [obj_type_id] + aligner.entity2hierarchy.get(obj_type_id, [])\n",
    "    updated_obj = 'None'\n",
    "    \n",
    "    # do not change time or quantity entities\n",
    "    similar_objects = aligner.retrieve_entity_by_type(\n",
    "        entity=triplet['object'],\n",
    "        entity_type=obj_type\n",
    "    )\n",
    "    input_3d_step = extractor.refine_entity(\n",
    "        text=text,\n",
    "        triplet=triplet,\n",
    "        candidates=similar_objects,\n",
    "        is_object=True\n",
    "    )\n",
    "\n",
    "    system_prompt = input_3d_step['system_prompt']\n",
    "    user_prompt = input_3d_step['user_prompt']\n",
    "\n",
    "    COST = 2 * calc.calculate_input_cost(system_prompt) + calc.calculate_input_cost(user_prompt)\n",
    "\n",
    "    return COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:02<00:00, 12.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.428905"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COST = 0\n",
    "for sample_id in id2sample:\n",
    "\n",
    "    sample = id2sample[sample_id]\n",
    "\n",
    "    aligner = Aligner()\n",
    "    model_name = 'gpt-4o'\n",
    "    extractor = LLMTripletExtractor(model=model_name)\n",
    "    \n",
    "    texts = [item['paragraph_text'] for item in sample['paragraphs']]\n",
    "    for text in tqdm(texts):        \n",
    "        COST += extract_triplets(text)\n",
    "\n",
    "    break\n",
    "\n",
    "COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.68"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.42/50*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.62*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18+30+30+2+2+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "92/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 136178.70it/s]\n"
     ]
    }
   ],
   "source": [
    "toks = []\n",
    "for sample_id in id2sample:\n",
    "\n",
    "    sample = id2sample[sample_id]\n",
    "    \n",
    "    texts = [item['paragraph_text'] for item in sample['paragraphs']]\n",
    "    for text in tqdm(texts):        \n",
    "        toks.extend(text.split())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
